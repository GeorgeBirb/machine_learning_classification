{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "\n",
    "# Model 4\n",
    "\n",
    "- get_dummies categorical data(`EDUCATION`,`MARRIAGE`,`SEX`,'PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6').\n",
    "- exclude features `BILL_ATM2`, ..., `BILL_ATM6`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries/packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General libraries ###\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import graphviz \n",
    "from graphviz import Source\n",
    "from IPython.display import SVG\n",
    "import os\n",
    "\n",
    "##################################\n",
    "\n",
    "### ML Models ###\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "# from sklearn.tree.export import export_text\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "##################################\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "### Metrics ###\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score,confusion_matrix, mean_squared_error, mean_absolute_error, classification_report, roc_auc_score, roc_curve, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Load and clean the data\n",
    "\n",
    "In this section we will load the data from the csv file and check for any \"impurities\", such as null values or duplicate rows. If any of these will appear, we will remove them from the data set. We will also plot the correlations of the class column with all the other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000 entries, 0 to 29999\n",
      "Data columns (total 25 columns):\n",
      "ID           30000 non-null int64\n",
      "LIMIT_BAL    30000 non-null int64\n",
      "SEX          30000 non-null int64\n",
      "EDUCATION    30000 non-null int64\n",
      "MARRIAGE     30000 non-null int64\n",
      "AGE          30000 non-null int64\n",
      "PAY_1        30000 non-null int64\n",
      "PAY_2        30000 non-null int64\n",
      "PAY_3        30000 non-null int64\n",
      "PAY_4        30000 non-null int64\n",
      "PAY_5        30000 non-null int64\n",
      "PAY_6        30000 non-null int64\n",
      "BILL_AMT1    30000 non-null int64\n",
      "BILL_AMT2    30000 non-null int64\n",
      "BILL_AMT3    30000 non-null int64\n",
      "BILL_AMT4    30000 non-null int64\n",
      "BILL_AMT5    30000 non-null int64\n",
      "BILL_AMT6    30000 non-null int64\n",
      "PAY_AMT1     30000 non-null int64\n",
      "PAY_AMT2     30000 non-null int64\n",
      "PAY_AMT3     30000 non-null int64\n",
      "PAY_AMT4     30000 non-null int64\n",
      "PAY_AMT5     30000 non-null int64\n",
      "PAY_AMT6     30000 non-null int64\n",
      "dpnm         30000 non-null int64\n",
      "dtypes: int64(25)\n",
      "memory usage: 5.7 MB\n"
     ]
    }
   ],
   "source": [
    "# Load the data.\n",
    "data = pd.read_csv('default of credit card clients.csv')\n",
    "\n",
    "# Information\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the `ID` column is for indexing purposes only, we remove it from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace value '0' with value '3'.\n",
    "data['MARRIAGE'] = data['MARRIAGE'].replace(0, 3)\n",
    "\n",
    "# Replace values '0','5' and '6' with value '4'.\n",
    "data['EDUCATION'] = data['EDUCATION'].replace([0, 5, 6], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Drop \"ID\" column.\n",
    "data = data.drop(['ID'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check for duplicate rows. If any, we remove them from the data set, since they provide only reduntant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 35 duplicate rows in the data set.\n",
      "The duplicate rows were removed.\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate rows.\n",
    "print(f\"There are {data.duplicated().sum()} duplicate rows in the data set.\")\n",
    "\n",
    "# Remove duplicate rows.\n",
    "data = data.drop_duplicates()\n",
    "print(\"The duplicate rows were removed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also check for null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 cells with null values in the data set.\n"
     ]
    }
   ],
   "source": [
    "# Check for null values.\n",
    "print(\n",
    "    f\"There are {data.isna().any().sum()} cells with null values in the data set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the plot of the correlation matrix for the data set."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(data.corr(), annot=True, cmap='rainbow',\n",
    "            cbar=False, linewidth=0.5, fmt='.2f')\n",
    "plt.title('Correlation Matrix')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import plotly.figure_factory as ff\n",
    "corrs = data.corr()\n",
    "figure = ff.create_annotated_heatmap(\n",
    "    z=corrs.values,\n",
    "    x=list(corrs.columns),\n",
    "    y=list(corrs.index),\n",
    "    annotation_text=corrs.round(2).values,\n",
    "    showscale=True, colorscale='Viridis')\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Pre-processing\n",
    "\n",
    "In this part we prepare our data for our models. This means that we choose the columns that will be our independed variables and which column the class that we want to predict. Once we are done with that, we split our data into train and test sets and perfom a standardization upon them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHot encoder for columns 'EDUCATION','MARRIAGE','SEX','PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6' .\n",
    "data = pd.get_dummies(\n",
    "    data, columns=['EDUCATION','MARRIAGE','SEX','PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select feature and class column.\n",
    "X = data.drop(columns=['dpnm', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']) # , 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6'\n",
    "y = data['dpnm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = X.copy()\n",
    "X_clust = X.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best k for CV accuracy:17, 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = SelectKBest(k=23).fit_transform(X_, y)\n",
    "# Split to train and test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_, y, test_size=0.3, random_state=25)\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "# Initialize a Logistic Regression estimator.\n",
    "logreg = LogisticRegression(random_state=25, n_jobs=-1)\n",
    "\n",
    "# Train the estimator.\n",
    "logreg.fit(X_train, y_train)\n",
    "# Make predictions.\n",
    "log_pred = logreg.predict(X_test)\n",
    "# CV accuracy\n",
    "cv_logreg = cross_validate(logreg, X_, y, scoring=scoring, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform K-means clustering analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clust = SelectKBest(k=17).fit_transform(X_clust, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_clust)\n",
    "X_clust = sc.transform(X_clust)\n",
    "X_clust = pd.DataFrame(X_clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=2, random_state=25).fit(X_clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clust['cluster'] = model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to train and test sets for X_clust.\n",
    "X_clust_train, X_clust_test, y_clust_train, y_clust_test = train_test_split(\n",
    "    X_clust, y, test_size=0.3, random_state=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization for X_clust\n",
    "scaler_ = StandardScaler()\n",
    "X_clust_train = scaler_.fit_transform(X_clust_train)\n",
    "X_clust_test = scaler_.transform(X_clust_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Modeling\n",
    "\n",
    "In this section we build and try 3 models:\n",
    " - Logistic Regression\n",
    " - Decision tree\n",
    " - Neural network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "# # Initialize a Logistic Regression estimator.\n",
    "# logreg = LogisticRegression(random_state=25, n_jobs=-1)\n",
    "\n",
    "# # Train the estimator.\n",
    "# logreg.fit(X_train, y_train)\n",
    "# # Make predictions.\n",
    "# log_pred = logreg.predict(X_test)\n",
    "# # CV accuracy\n",
    "# cv_logreg = cross_validate(logreg, X, y, scoring=scoring, cv=10)\n",
    "\n",
    "\n",
    "# Initialize a Logistic Regression estimator.\n",
    "logreg_clust = LogisticRegression(random_state=25, n_jobs=-1)\n",
    "# Train the estimator for clustering.\n",
    "logreg_clust.fit(X_clust_train, y_clust_train)\n",
    "# Make predictions for clustering.\n",
    "log_clust_pred = logreg_clust.predict(X_clust_test)\n",
    "\n",
    "cv_logreg_clust = cross_validate(\n",
    "    logreg_clust, X_clust, y, scoring=scoring, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CV Accuracy</th>\n",
       "      <th>CV Precision</th>\n",
       "      <th>CV Recall</th>\n",
       "      <th>CV F1</th>\n",
       "      <th>CV AUC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Models</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.780</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression w/ clustering</th>\n",
       "      <td>0.819</td>\n",
       "      <td>0.677</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   CV Accuracy  CV Precision  CV Recall  \\\n",
       "Models                                                                    \n",
       "Logistic Regression                      0.780         0.068      0.009   \n",
       "Logistic Regression w/ clustering        0.819         0.677      0.349   \n",
       "\n",
       "                                   CV F1  CV AUC  \n",
       "Models                                            \n",
       "Logistic Regression                0.016   0.638  \n",
       "Logistic Regression w/ clustering  0.460   0.758  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {\n",
    "    'Models': ['Logistic Regression', 'Logistic Regression w/ clustering'],\n",
    "    'CV Accuracy': [cv_logreg['test_accuracy'].mean(), cv_logreg_clust['test_accuracy'].mean()],\n",
    "    'CV Precision': [cv_logreg['test_precision'].mean(), cv_logreg_clust['test_precision'].mean()],\n",
    "    'CV Recall': [cv_logreg['test_recall'].mean(), cv_logreg_clust['test_recall'].mean()],\n",
    "    'CV F1': [cv_logreg['test_f1'].mean(), cv_logreg_clust['test_f1'].mean()],\n",
    "    'CV AUC': [cv_logreg['test_roc_auc'].mean(), cv_logreg_clust['test_roc_auc'].mean()]\n",
    "}\n",
    "\n",
    "results = pd.DataFrame(data=d).round(3).set_index('Models')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=30, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=25, splitter='best')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a decision tree estimator.\n",
    "tr = tree.DecisionTreeClassifier(\n",
    "    max_depth=30, criterion='gini', random_state=25)\n",
    "\n",
    "# Train the estimator.\n",
    "tr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions.\n",
    "tr_pred = tr.predict(X_test)\n",
    "\n",
    "# CV accuracy.\n",
    "cv_tr = cross_validate(tr, X_, y, scoring=scoring, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a decision tree estimator.\n",
    "tr_clust = tree.DecisionTreeClassifier(\n",
    "    max_depth=30, criterion='gini', random_state=25)\n",
    "\n",
    "# Train the estimator.\n",
    "tr_clust.fit(X_clust_train, y_clust_train)\n",
    "\n",
    "# Make predictions.\n",
    "tr_clust_pred = tr_clust.predict(X_clust_test)\n",
    "\n",
    "# CV accuracy.\n",
    "cv_tr_clust = cross_validate(tr_clust, X_clust, y, scoring=scoring, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CV Accuracy</th>\n",
       "      <th>CV Precision</th>\n",
       "      <th>CV Recall</th>\n",
       "      <th>CV F1</th>\n",
       "      <th>CV AUC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Models</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.758</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree w/ clustering</th>\n",
       "      <td>0.800</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             CV Accuracy  CV Precision  CV Recall  CV F1  \\\n",
       "Models                                                                     \n",
       "Decision Tree                      0.758         0.442      0.357  0.395   \n",
       "Decision Tree w/ clustering        0.800         0.586      0.330  0.422   \n",
       "\n",
       "                             CV AUC  \n",
       "Models                               \n",
       "Decision Tree                 0.638  \n",
       "Decision Tree w/ clustering   0.682  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {\n",
    "    'Models': ['Decision Tree', 'Decision Tree w/ clustering'],\n",
    "    'CV Accuracy': [cv_tr['test_accuracy'].mean(), cv_tr_clust['test_accuracy'].mean()],\n",
    "    'CV Precision': [cv_tr['test_precision'].mean(), cv_tr_clust['test_precision'].mean()],\n",
    "    'CV Recall': [cv_tr['test_recall'].mean(), cv_tr_clust['test_recall'].mean()],\n",
    "    'CV F1': [cv_tr['test_f1'].mean(), cv_tr_clust['test_f1'].mean()],\n",
    "    'CV AUC': [cv_tr['test_roc_auc'].mean(), cv_tr_clust['test_roc_auc'].mean()]\n",
    "}\n",
    "\n",
    "results = pd.DataFrame(data=d).round(3).set_index('Models')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(12, 5), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=1000,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=25, shuffle=True, solver='adam',\n",
       "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a Multi-layer Perceptron classifier.\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(12, 5), max_iter=1000,\n",
    "                    random_state=25, shuffle=True, verbose=False)\n",
    "\n",
    "# Train the classifier.\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions.\n",
    "mlp_pred = mlp.predict(X_test)\n",
    "\n",
    "# CV accuracy\n",
    "cv_mlp = cross_validate(tr, X_, y, scoring=scoring, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a decision tree estimator.\n",
    "mlp_clust = MLPClassifier(hidden_layer_sizes=(12, 5), max_iter=1000,\n",
    "                    random_state=25, shuffle=True, verbose=False)\n",
    "\n",
    "# Train the estimator.\n",
    "mlp_clust.fit(X_clust_train, y_clust_train)\n",
    "\n",
    "# Make predictions.\n",
    "mlp_clust_pred = mlp_clust.predict(X_clust_test)\n",
    "\n",
    "# CV accuracy.\n",
    "cv_mlp_clust = cross_validate(mlp_clust, X_clust, y, scoring=scoring, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CV Accuracy</th>\n",
       "      <th>CV Precision</th>\n",
       "      <th>CV Recall</th>\n",
       "      <th>CV F1</th>\n",
       "      <th>CV AUC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Models</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MLP</th>\n",
       "      <td>0.758</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP w/ clustering</th>\n",
       "      <td>0.820</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.366</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   CV Accuracy  CV Precision  CV Recall  CV F1  CV AUC\n",
       "Models                                                                \n",
       "MLP                      0.758         0.442      0.357  0.395   0.638\n",
       "MLP w/ clustering        0.820         0.672      0.366  0.473   0.765"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {\n",
    "    'Models': ['MLP', 'MLP w/ clustering'],\n",
    "    'CV Accuracy': [cv_mlp['test_accuracy'].mean(), cv_mlp_clust['test_accuracy'].mean()],\n",
    "    'CV Precision': [cv_mlp['test_precision'].mean(), cv_mlp_clust['test_precision'].mean()],\n",
    "    'CV Recall': [cv_mlp['test_recall'].mean(), cv_mlp_clust['test_recall'].mean()],\n",
    "    'CV F1': [cv_mlp['test_f1'].mean(), cv_mlp_clust['test_f1'].mean()],\n",
    "    'CV AUC': [cv_mlp['test_roc_auc'].mean(), cv_mlp_clust['test_roc_auc'].mean()]\n",
    "}\n",
    "\n",
    "results = pd.DataFrame(data=d).round(3).set_index('Models')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper results\n",
    "\n",
    "\n",
    "### 1) The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. Expert Systems with Applications 36 (2009) 2473–2480\n",
    "\n",
    "|                     | Error rate | Accuracy |\n",
    "|---------------------|:----------:|:--------:|\n",
    "| Logistic Regression |    0.18    |   0.82   |\n",
    "| Decision tree       |    0.17    |   0.83   |\n",
    "| Neural Network      |    0.17    |   0.83   |\n",
    "\n",
    "### 2) Liu,  R.L.  (2018) Machine  Learning  Approaches  to  Predict Default  of  Credit  Card  Clients. Modern Economy, 9, 1828-1838. \n",
    "\n",
    "|                | Accuracy |   F1   |\n",
    "|----------------|:--------:|:------:|\n",
    "|  Decision tree |  0.7973  | 0.4912 |\n",
    "| Neural Network |  0.8227  | 0.4593 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
